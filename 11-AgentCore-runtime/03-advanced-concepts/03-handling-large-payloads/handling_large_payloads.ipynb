{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52dc9b17-1182-44b3-bebf-ae2f508675d3",
   "metadata": {},
   "source": [
    "# Handling Large Multi-Modal Payloads in AgentCore Runtime\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how Amazon Bedrock AgentCore Runtime handles large payloads up to 100MB, including multi-modal content such as Excel files and images. AgentCore Runtime is designed to process rich media content and large datasets seamlessly.\n",
    "\n",
    "### Tutorial Details\n",
    "\n",
    "|Information| Details|\n",
    "|:--------------------|:---------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Large Payload & Multi-Modal Processing|\n",
    "| Agent type          | Single         |\n",
    "| Agentic Framework   | Strands Agents |\n",
    "| LLM model           | Anthropic Claude Haiku 4.5 |\n",
    "| Tutorial components | Large File Processing, Image Analysis, Excel Data Processing |\n",
    "| Tutorial vertical   | Data Analysis & Multi-Modal AI                                                   |\n",
    "| Example complexity  | Intermediate                                                                     |\n",
    "| SDK used            | Amazon BedrockAgentCore Python SDK|\n",
    "\n",
    "### Key Features\n",
    "\n",
    "* **Large Payload Support**: Process files up to 100MB in size\n",
    "* **Multi-Modal Processing**: Handle Excel files, images, and text simultaneously\n",
    "* **Data Analysis**: Extract insights from structured data and visual content\n",
    "* **Base64 Encoding**: Secure transmission of binary data through JSON payloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a676f58ecf52b42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Python 3.10+\n",
    "* AWS credentials configured\n",
    "* Docker running\n",
    "* Sample Excel file and image for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.4.0 requires nvidia-ml-py3<8.0,>=7.352.0, which is not installed.\n",
      "dash 2.18.1 requires dash-core-components==2.0.0, which is not installed.\n",
      "dash 2.18.1 requires dash-html-components==2.0.0, which is not installed.\n",
      "dash 2.18.1 requires dash-table==5.0.0, which is not installed.\n",
      "jupyter-ai 2.31.6 requires faiss-cpu!=1.8.0.post0,<2.0.0,>=1.8.0, which is not installed.\n",
      "sagemaker-studio 1.1.1 requires pydynamodb>=0.7.4, which is not installed.\n",
      "aiobotocore 2.22.0 requires botocore<1.37.4,>=1.37.2, but you have botocore 1.42.7 which is incompatible.\n",
      "amazon-sagemaker-jupyter-ai-q-developer 1.2.8 requires numpy<=2.0.1, but you have numpy 2.3.5 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.4 requires numpy<2, but you have numpy 2.3.5 which is incompatible.\n",
      "autogluon-multimodal 1.4.0 requires jsonschema<4.24,>=4.18, but you have jsonschema 4.25.1 which is incompatible.\n",
      "autogluon-multimodal 1.4.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.57.1 which is incompatible.\n",
      "autogluon-timeseries 1.4.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.57.1 which is incompatible.\n",
      "catboost 1.2.7 requires numpy<2.0,>=1.16.0, but you have numpy 2.3.5 which is incompatible.\n",
      "dash 2.18.1 requires Flask<3.1,>=1.0.4, but you have flask 3.1.2 which is incompatible.\n",
      "dash 2.18.1 requires Werkzeug<3.1, but you have werkzeug 3.1.3 which is incompatible.\n",
      "databricks-sdk 0.73.0 requires protobuf!=5.26.*,!=5.27.*,!=5.28.*,!=5.29.0,!=5.29.1,!=5.29.2,!=5.29.3,!=5.29.4,!=6.30.0,!=6.30.1,!=6.31.0,<7.0,>=4.25.8, but you have protobuf 5.28.3 which is incompatible.\n",
      "fastapi 0.121.1 requires starlette<0.50.0,>=0.40.0, but you have starlette 0.50.0 which is incompatible.\n",
      "flake8 7.1.2 requires pycodestyle<2.13.0,>=2.12.0, but you have pycodestyle 2.14.0 which is incompatible.\n",
      "gluonts 0.16.2 requires numpy<2.2,>=1.16, but you have numpy 2.3.5 which is incompatible.\n",
      "jupyter-scheduler 2.11.0 requires pytz<=2024.2,>=2023.3, but you have pytz 2025.2 which is incompatible.\n",
      "mlflow 2.22.0 requires packaging<25, but you have packaging 25.0 which is incompatible.\n",
      "mlflow-skinny 2.22.0 requires packaging<25, but you have packaging 25.0 which is incompatible.\n",
      "sagemaker 2.245.0 requires attrs<24,>=23.1.0, but you have attrs 25.4.0 which is incompatible.\n",
      "sagemaker 2.245.0 requires importlib-metadata<7.0,>=1.4.0, but you have importlib-metadata 8.7.0 which is incompatible.\n",
      "sagemaker 2.245.0 requires numpy==1.26.4, but you have numpy 2.3.5 which is incompatible.\n",
      "sagemaker 2.245.0 requires packaging<25,>=23.0, but you have packaging 25.0 which is incompatible.\n",
      "sagemaker-studio-analytics-extension 0.2.2 requires sparkmagic==0.22.0, but you have sparkmagic 0.21.0 which is incompatible.\n",
      "snowflake-connector-python 3.17.4 requires cffi<2.0.0,>=1.9, but you have cffi 2.0.0 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.3.3 which is incompatible.\n",
      "sagemaker-studio 1.1.1 requires numpy<2.3.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932110e6-fca6-47b6-b7c5-c4714a866a80",
   "metadata": {},
   "source": [
    "## Create Sample Data Files\n",
    "\n",
    "Let's create sample Excel and image files to demonstrate large payload handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "create-sample-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file size: 0.05 MB\n",
      "Image file size: 0.01 MB\n",
      "Total payload size: 0.05 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "\n",
    "# Create a large Excel file with sample sales data\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Date': pd.date_range('2023-01-01', periods=1000, freq='h'),\n",
    "    'Product': np.random.choice(['Widget A', 'Widget B', 'Widget C', 'Gadget X', 'Gadget Y'], 1000),\n",
    "    'Sales': np.random.randint(1, 1000, 1000),\n",
    "    'Revenue': np.random.uniform(10.0, 5000.0, 1000),\n",
    "    'Region': np.random.choice(['North', 'South', 'East', 'West'], 1000),\n",
    "    'Customer_ID': np.random.randint(1000, 9999, 1000)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel('large_sales_data.xlsx', index=False)\n",
    "\n",
    "# Create a sample chart image\n",
    "img = Image.new('RGB', (600, 500), color='white')\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "# Draw a simple bar chart\n",
    "products = ['Widget A', 'Widget B', 'Widget C', 'Gadget X', 'Gadget Y']\n",
    "values = [250, 180, 320, 150, 280]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
    "\n",
    "max_value = max(values)\n",
    "bar_width = 120\n",
    "start_x = 100\n",
    "\n",
    "for i, (product, value, color) in enumerate(zip(products, values, colors)):\n",
    "    x = start_x + i * (bar_width + 20)\n",
    "    height = int((value / max_value) * 400)\n",
    "    y = 500 - height\n",
    "    \n",
    "    # Draw bar\n",
    "    draw.rectangle([x, y, x + bar_width, 500], fill=color)\n",
    "    \n",
    "    # Add labels (simplified without font)\n",
    "    draw.text((x + 10, 510), product[:8], fill='black')\n",
    "    draw.text((x + 10, y - 20), str(value), fill='black')\n",
    "\n",
    "draw.text((300, 50), 'Sales Performance by Product', fill='black')\n",
    "img.save('sales_chart.png')\n",
    "\n",
    "# Check file sizes\n",
    "excel_size = os.path.getsize('large_sales_data.xlsx') / (1024 * 1024)  # MB\n",
    "image_size = os.path.getsize('sales_chart.png') / (1024 * 1024)  # MB\n",
    "\n",
    "print(f\"Excel file size: {excel_size:.2f} MB\")\n",
    "print(f\"Image file size: {image_size:.2f} MB\")\n",
    "print(f\"Total payload size: {excel_size + image_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-code",
   "metadata": {},
   "source": [
    "## Create Multi-Modal Agent\n",
    "\n",
    "Let's create an agent that can process both Excel files and images from large payloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b845b32-a03e-45c2-a2f0-2afba8069f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing multimodal_data_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile multimodal_data_agent.py\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "import pandas as pd\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "from bedrock_agentcore.runtime import BedrockAgentCoreApp\n",
    "\n",
    "app = BedrockAgentCoreApp()\n",
    "\n",
    "# Initialize the model and agent\n",
    "model_id = \"global.anthropic.claude-haiku-4-5-20251001-v1:0\"\n",
    "model = BedrockModel(\n",
    "    model_id=model_id,\n",
    "    max_tokens=16000\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    system_prompt=\"\"\"\n",
    "    You are a data analysis assistant that can process large Excel files and images.\n",
    "    When given multi-modal data, analyze both the structured data and visual content,\n",
    "    then provide comprehensive insights combining both data sources.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "@app.entrypoint\n",
    "def multimodal_data_processor(payload, context):\n",
    "    \"\"\"\n",
    "    Process large multi-modal payloads containing Excel data and images.\n",
    "    \n",
    "    Args:\n",
    "        payload: Contains prompt, excel_data (base64), image_data (base64)\n",
    "        context: Runtime context information\n",
    "    \n",
    "    Returns:\n",
    "        str: Analysis results from both data sources\n",
    "    \"\"\"\n",
    "    prompt = payload.get(\"prompt\", \"Analyze the provided data.\")\n",
    "    excel_data = payload.get(\"excel_data\", \"\")\n",
    "    image_data = payload.get(\"image_data\", \"\")\n",
    "    \n",
    "    print(f\"=== Large Payload Processing ===\")\n",
    "    print(f\"Session ID: {context.session_id}\")\n",
    "    \n",
    "    if excel_data:\n",
    "        print(f\"Excel data size: {len(excel_data) / 1024 / 1024:.2f} MB\")\n",
    "    if image_data:\n",
    "        print(f\"Image data size: {len(image_data) / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"Excel data {excel_data}\")\n",
    "    print(f\"Image data {image_data}\")\n",
    "    print(f\"=== Processing Started ===\")\n",
    "    # Decode base64 to bytes\n",
    "    excel_bytes = base64.b64decode(excel_data)\n",
    "    # Decode base64 to bytes\n",
    "    image_bytes = base64.b64decode(image_data)\n",
    "    \n",
    "    # Enhanced prompt with data context\n",
    "    enhanced_prompt = f\"\"\"{prompt}\n",
    "    Please analyze both data sources and provide insights.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = agent(\n",
    "        [{\n",
    "            \"document\": {\n",
    "                \"format\": \"xlsx\",\n",
    "                \"name\": \"excel_data\",\n",
    "                \"source\": {\n",
    "                    \"bytes\": excel_bytes\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"format\": \"png\",\n",
    "                \"source\": {\n",
    "                    \"bytes\": image_bytes\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"text\": enhanced_prompt\n",
    "        }]\n",
    "    )\n",
    "    return response.message['content'][0]['text']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-infrastructure",
   "metadata": {},
   "source": [
    "## Setup Infrastructure and Deploy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e79eba2-ca59-463f-9ebf-56e362d7ae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrypoint parsed: file=/home/sagemaker-user/amazon-bedrock-agentcore-workshop/strand-agent-samples/11-AgentCore-runtime/03-advanced-concepts/03-handling-large-payloads/multimodal_data_agent.py, bedrock_agentcore_name=multimodal_data_agent\n",
      "Configuring BedrockAgentCore agent: multimodal_data_agent\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">‚ö†Ô∏è  [WARNING] Platform mismatch: Current system is </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">'linux/amd64'</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\"> but Bedrock AgentCore requires </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">'linux/arm64'</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">.</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">For deployment options and workarounds, see: </span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/getting-started-custom.html</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;4;33m‚ö†Ô∏è  \u001b[0m\u001b[1;4;33m[\u001b[0m\u001b[1;4;33mWARNING\u001b[0m\u001b[1;4;33m]\u001b[0m\u001b[1;4;33m Platform mismatch: Current system is \u001b[0m\u001b[4;32m'linux/amd64'\u001b[0m\u001b[1;4;33m but Bedrock AgentCore requires \u001b[0m\u001b[4;32m'linux/arm64'\u001b[0m\u001b[1;4;33m.\u001b[0m\n",
       "\u001b[1;4;33mFor deployment options and workarounds, see: \u001b[0m\n",
       "\u001b[4;94mhttps://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/getting-started-custom.html\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generated .dockerignore\n",
      "Generated Dockerfile: /home/sagemaker-user/amazon-bedrock-agentcore-workshop/strand-agent-samples/11-AgentCore-runtime/03-advanced-concepts/03-handling-large-payloads/Dockerfile\n",
      "Generated .dockerignore: /home/sagemaker-user/amazon-bedrock-agentcore-workshop/strand-agent-samples/11-AgentCore-runtime/03-advanced-concepts/03-handling-large-payloads/.dockerignore\n",
      "Setting 'multimodal_data_agent' as default agent\n",
      "Bedrock AgentCore configured: /home/sagemaker-user/amazon-bedrock-agentcore-workshop/strand-agent-samples/11-AgentCore-runtime/03-advanced-concepts/03-handling-large-payloads/.bedrock_agentcore.yaml\n",
      "üöÄ CodeBuild mode: building in cloud (RECOMMENDED - DEFAULT)\n",
      "   ‚Ä¢ Build ARM64 containers in the cloud with CodeBuild\n",
      "   ‚Ä¢ No local Docker required\n",
      "üí° Available deployment modes:\n",
      "   ‚Ä¢ runtime.launch()                           ‚Üí CodeBuild (current)\n",
      "   ‚Ä¢ runtime.launch(local=True)                 ‚Üí Local development\n",
      "   ‚Ä¢ runtime.launch(local_build=True)           ‚Üí Local build + cloud deploy (NEW)\n",
      "Starting CodeBuild ARM64 deployment for agent 'multimodal_data_agent' to account 455933813601 (us-west-2)\n",
      "Setting up AWS resources (ECR repository, execution roles)...\n",
      "Getting or creating ECR repository for agent: multimodal_data_agent\n",
      "‚úÖ ECR repository available: 455933813601.dkr.ecr.us-west-2.amazonaws.com/bedrock-agentcore-multimodal_data_agent\n",
      "Getting or creating execution role for agent: multimodal_data_agent\n",
      "Using AWS region: us-west-2, account ID: 455933813601\n",
      "Role name: AmazonBedrockAgentCoreSDKRuntime-us-west-2-069e2d958e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository doesn't exist, creating new ECR repository: bedrock-agentcore-multimodal_data_agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Role doesn't exist, creating new execution role: AmazonBedrockAgentCoreSDKRuntime-us-west-2-069e2d958e\n",
      "Starting execution role creation process for agent: multimodal_data_agent\n",
      "‚úì Role creating: AmazonBedrockAgentCoreSDKRuntime-us-west-2-069e2d958e\n",
      "Creating IAM role: AmazonBedrockAgentCoreSDKRuntime-us-west-2-069e2d958e\n",
      "‚úì Role created: arn:aws:iam::455933813601:role/AmazonBedrockAgentCoreSDKRuntime-us-west-2-069e2d958e\n",
      "‚úì Execution policy attached: BedrockAgentCoreRuntimeExecutionPolicy-multimodal_data_agent\n",
      "Role creation complete and ready for use with Bedrock AgentCore\n",
      "‚úÖ Execution role available: arn:aws:iam::455933813601:role/AmazonBedrockAgentCoreSDKRuntime-us-west-2-069e2d958e\n",
      "Preparing CodeBuild project and uploading source...\n",
      "Getting or creating CodeBuild execution role for agent: multimodal_data_agent\n",
      "Role name: AmazonBedrockAgentCoreSDKCodeBuild-us-west-2-069e2d958e\n",
      "CodeBuild role doesn't exist, creating new role: AmazonBedrockAgentCoreSDKCodeBuild-us-west-2-069e2d958e\n",
      "Creating IAM role: AmazonBedrockAgentCoreSDKCodeBuild-us-west-2-069e2d958e\n",
      "‚úì Role created: arn:aws:iam::455933813601:role/AmazonBedrockAgentCoreSDKCodeBuild-us-west-2-069e2d958e\n",
      "Attaching inline policy: CodeBuildExecutionPolicy to role: AmazonBedrockAgentCoreSDKCodeBuild-us-west-2-069e2d958e\n",
      "‚úì Policy attached: CodeBuildExecutionPolicy\n",
      "Waiting for IAM role propagation...\n",
      "CodeBuild execution role creation complete: arn:aws:iam::455933813601:role/AmazonBedrockAgentCoreSDKCodeBuild-us-west-2-069e2d958e\n",
      "Using .dockerignore with 44 patterns\n",
      "Uploaded source to S3: multimodal_data_agent/source.zip\n",
      "Created CodeBuild project: bedrock-agentcore-multimodal_data_agent-builder\n",
      "Starting CodeBuild build (this may take several minutes)...\n",
      "Starting CodeBuild monitoring...\n",
      "üîÑ QUEUED started (total: 0s)\n",
      "‚úÖ QUEUED completed in 1.0s\n",
      "üîÑ PROVISIONING started (total: 1s)\n",
      "‚úÖ PROVISIONING completed in 9.3s\n",
      "üîÑ DOWNLOAD_SOURCE started (total: 10s)\n",
      "‚úÖ DOWNLOAD_SOURCE completed in 2.1s\n",
      "üîÑ BUILD started (total: 12s)\n",
      "‚úÖ BUILD completed in 21.7s\n",
      "üîÑ POST_BUILD started (total: 34s)\n",
      "‚úÖ POST_BUILD completed in 15.5s\n",
      "üîÑ COMPLETED started (total: 50s)\n",
      "‚úÖ COMPLETED completed in 1.0s\n",
      "üéâ CodeBuild completed successfully in 0m 50s\n",
      "CodeBuild completed successfully\n",
      "‚úÖ CodeBuild project configuration saved\n",
      "Deploying to Bedrock AgentCore...\n",
      "‚úÖ Agent created/updated: arn:aws:bedrock-agentcore:us-west-2:455933813601:runtime/multimodal_data_agent-yHV4tcFP8h\n",
      "Observability is enabled, configuring Transaction Search...\n",
      "Created/updated CloudWatch Logs resource policy\n",
      "Transaction Search configuration failed: An error occurred (AccessDeniedException) when calling the UpdateTraceSegmentDestination operation: User: arn:aws:sts::455933813601:assumed-role/workshop-studio-SageMakerExecutionRole-sN3XOhhTpzTP/SageMaker is not authorized to perform: xray:UpdateTraceSegmentDestination because no identity-based policy allows the xray:UpdateTraceSegmentDestination action\n",
      "Agent launch will continue without Transaction Search\n",
      "üîç GenAI Observability Dashboard:\n",
      "   https://console.aws.amazon.com/cloudwatch/home?region=us-west-2#gen-ai-observability/agent-core\n",
      "Polling for endpoint to be ready...\n",
      "Agent endpoint: arn:aws:bedrock-agentcore:us-west-2:455933813601:runtime/multimodal_data_agent-yHV4tcFP8h/runtime-endpoint/DEFAULT\n",
      "Deployment completed successfully - Agent: arn:aws:bedrock-agentcore:us-west-2:455933813601:runtime/multimodal_data_agent-yHV4tcFP8h\n",
      "Built with CodeBuild: bedrock-agentcore-multimodal_data_agent-builder:5c501822-68e5-442e-931a-02c9644e5674\n",
      "Deployed to cloud: arn:aws:bedrock-agentcore:us-west-2:455933813601:runtime/multimodal_data_agent-yHV4tcFP8h\n",
      "ECR image: 455933813601.dkr.ecr.us-west-2.amazonaws.com/bedrock-agentcore-multimodal_data_agent\n",
      "üîç Agent logs available at:\n",
      "   /aws/bedrock-agentcore/runtimes/multimodal_data_agent-yHV4tcFP8h-DEFAULT --log-stream-name-prefix \"2025/12/11/\\[runtime-logs]\"\n",
      "   /aws/bedrock-agentcore/runtimes/multimodal_data_agent-yHV4tcFP8h-DEFAULT --log-stream-names \"otel-rt-logs\"\n",
      "üí° Tail logs with: aws logs tail /aws/bedrock-agentcore/runtimes/multimodal_data_agent-yHV4tcFP8h-DEFAULT --log-stream-name-prefix \"2025/12/11/\\[runtime-logs]\" --follow\n",
      "üí° Or view recent logs: aws logs tail /aws/bedrock-agentcore/runtimes/multimodal_data_agent-yHV4tcFP8h-DEFAULT --log-stream-name-prefix \"2025/12/11/\\[runtime-logs]\" --since 1h\n"
     ]
    }
   ],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "from boto3.session import Session\n",
    "\n",
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "agentcore_runtime = Runtime()\n",
    "\n",
    "response = agentcore_runtime.configure(\n",
    "    entrypoint=\"multimodal_data_agent.py\",\n",
    "    auto_create_execution_role=True,\n",
    "    auto_create_ecr=True,\n",
    "    requirements_file=\"requirements.txt\",\n",
    "    region=region,\n",
    "    agent_name=\"multimodal_data_agent\"\n",
    ")\n",
    "\n",
    "launch_result = agentcore_runtime.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40ca45f2-64e8-4b05-96b1-3061b26e20f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM ghcr.io/astral-sh/uv:python3.12-bookworm-slim\n",
      "WORKDIR /app\n",
      "\n",
      "# Configure UV for container environment\n",
      "ENV UV_SYSTEM_PYTHON=1 UV_COMPILE_BYTECODE=1\n",
      "\n",
      "\n",
      "\n",
      "COPY requirements.txt requirements.txt\n",
      "# Install from requirements file\n",
      "RUN uv pip install -r requirements.txt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RUN uv pip install aws-opentelemetry-distro>=0.10.1\n",
      "\n",
      "\n",
      "# Set AWS region environment variable\n",
      "\n",
      "ENV AWS_REGION=us-west-2\n",
      "ENV AWS_DEFAULT_REGION=us-west-2\n",
      "\n",
      "\n",
      "# Signal that this is running in Docker for host binding logic\n",
      "ENV DOCKER_CONTAINER=1\n",
      "\n",
      "# Create non-root user\n",
      "RUN useradd -m -u 1000 bedrock_agentcore\n",
      "USER bedrock_agentcore\n",
      "\n",
      "EXPOSE 8080\n",
      "EXPOSE 8000\n",
      "\n",
      "# Copy entire project (respecting .dockerignore)\n",
      "COPY . .\n",
      "\n",
      "# Use the full module path\n",
      "\n",
      "CMD [\"opentelemetry-instrument\", \"python\", \"-m\", \"multimodal_data_agent\"]\n"
     ]
    }
   ],
   "source": [
    "cat Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wait-for-deployment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieved Bedrock AgentCore status for: multimodal_data_agent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final status: READY\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "status_response = agentcore_runtime.status()\n",
    "status = status_response.endpoint['status']\n",
    "end_status = ['READY', 'CREATE_FAILED', 'DELETE_FAILED', 'UPDATE_FAILED']\n",
    "\n",
    "while status not in end_status:\n",
    "    time.sleep(10)\n",
    "    status_response = agentcore_runtime.status()\n",
    "    status = status_response.endpoint['status']\n",
    "    print(f\"Deployment status: {status}\")\n",
    "\n",
    "print(f\"Final status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-large-payloads",
   "metadata": {},
   "source": [
    "## Test Large Multi-Modal Payloads\n",
    "\n",
    "Now let's test the agent with large payloads containing both Excel data and images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d909e42-e1a0-407f-84c2-3d16cc889cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Processing large multi-modal payload...\n",
      "üìã Session ID: ba810f50-3949-47f5-baf4-591d25397454\n",
      "üìÑ Excel size: 0.06 MB\n",
      "üñºÔ∏è Image size: 0.01 MB\n",
      "üì¶ Total payload: 0.07 MB\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"# Sales Performance Analysis: Data & Visual Insights\\n\\n## Overview\\nBased on the Excel dataset (Jan 1 - Feb 11, 2023) and the sales performance chart, here are the comprehensive findings:\\n\\n---\\n\\n## Key Sales Metrics by Product\\n\\n### Sales Volume (from chart):\\n- **Gadget X**: 250 units\\n- **Widget C**: 180 units\\n- **Widget B**: 320 units ‚≠ê (highest)\\n- **Widget A**: 150 units\\n\\n### Detailed Analysis:\\n\\n#### ü•á **Widget B - Top Performer**\\n- **Sales Volume**: 320 units (highest)\\n- **Market Position**: Clear leader in sales quantity\\n- **Consistency**: Strong performance across regions\\n- **Regional Strength**: Distributed well across East, North, South, and West regions\\n\\n#### ü•à **Gadget X - Strong Second**\\n- **Sales Volume**: 250 units\\n- **Performance**: Consistent high performer\\n- **Revenue Efficiency**: Generally good revenue per unit despite lower units than Widget B\\n- **Market Appeal**: Steady demand throughout the period\\n\\n#### ü•â **Widget C - Mid-Tier Performer**\\n- **Sales Volume**: 180 units\\n- **Performance**: Moderate sales with variable pricing\\n- **Notable**: High volatility in revenue per unit (ranging from $16 to $4,839)\\n\\n#### ‚ö†Ô∏è **Widget A - Underperformer**\\n- **Sales Volume**: 150 units (lowest)\\n- **Concern**: Lagging significantly behind competitors\\n- **Opportunity**: Potential growth area needing strategic focus\\n\\n---\\n\\n## Revenue Insights\\n\\n### From Data Analysis:\\n- **Average Revenue per Unit**:\\n  - Widget B: ~$2,573 (strong revenue conversion)\\n  - Gadget X: ~$2,241 (reliable)\\n  - Widget C: ~$2,108 (variable)\\n  - Widget A: ~$2,094 (needs improvement)\\n\\n### Revenue Efficiency:\\n- Despite Widget B having the most units, revenue consistency varies\\n- Gadget X maintains stable revenue performance\\n- Widget A needs pricing strategy review or product enhancement\\n\\n---\\n\\n## Regional Performance Trends\\n\\nAcross all regions (East, North, South, West):\\n- **No single region dominates** - sales are well-distributed\\n- **East & North** show slightly higher transaction volumes\\n- **Seasonal/Daily Patterns**: Some variation by hour of day, suggesting time-based demand fluctuations\\n\\n---\\n\\n## Strategic Recommendations\\n\\n### 1. **Maximize Widget B Success**\\n   - Dominant market leader‚Äîmaintain inventory and distribution\\n   - Investigate reasons for success for application to other products\\n\\n### 2. **Support Gadget X Growth**\\n   - Second strongest performer with stable revenue\\n   - Consider increased marketing to boost units sold\\n\\n### 3. **Revitalize Widget A**\\n   - Currently underperforming at 150 units\\n   - Conduct market research on customer preferences\\n   - Consider pricing adjustments or product improvements\\n\\n### 4. **Stabilize Widget C Revenue**\\n   - High volatility suggests inconsistent pricing or demand\\n   - Standardize pricing strategy for predictability\\n\\n### 5. **Regional Expansion**\\n   - All regions show balanced performance\\n   - Opportunity for targeted regional campaigns to boost underperforming areas\\n\\n---\\n\\n## Conclusion\\n\\nWidget B clearly dominates the portfolio, but a balanced approach focusing on Gadget X's reliability and Widget A's turnaround potential would strengthen overall business performance. The data suggests **February showed potential for sustained growth**, with consistent customer engagement across all regions.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "import uuid\n",
    "import json\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Encode files to base64\n",
    "with open('large_sales_data.xlsx', 'rb') as f:\n",
    "    excel_base64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "with open('sales_chart.png', 'rb') as f:\n",
    "    image_base64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "# Create large payload\n",
    "large_payload = {\n",
    "    \"prompt\": \"Analyze the sales data from the Excel file and correlate it with the chart image. Provide insights on sales performance and trends.\",\n",
    "    \"excel_data\": excel_base64,\n",
    "    \"image_data\": image_base64\n",
    "}\n",
    "\n",
    "session_id = str(uuid.uuid4())\n",
    "print(f\"üìä Processing large multi-modal payload...\")\n",
    "print(f\"üìã Session ID: {session_id}\")\n",
    "print(f\"üìÑ Excel size: {len(excel_base64) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"üñºÔ∏è Image size: {len(image_base64) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"üì¶ Total payload: {len(json.dumps(large_payload)) / 1024 / 1024:.2f} MB\\n\")\n",
    "\n",
    "# Invoke agent with large payload\n",
    "invoke_response = agentcore_runtime.invoke(\n",
    "    large_payload,\n",
    "    session_id=session_id\n",
    ")\n",
    "final_response = \"\"\n",
    "for r in invoke_response['response']:\n",
    "    final_response += r\n",
    "response_data = final_response\n",
    "display(Markdown(response_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## Cleanup Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cleanup-resources",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleanup completed!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Clean up AWS resources\n",
    "agentcore_control_client = boto3.client('bedrock-agentcore-control', region_name=region)\n",
    "ecr_client = boto3.client('ecr', region_name=region)\n",
    "\n",
    "# Delete AgentCore Runtime\n",
    "runtime_delete_response = agentcore_control_client.delete_agent_runtime(\n",
    "    agentRuntimeId=launch_result.agent_id\n",
    ")\n",
    "\n",
    "# Delete ECR repository\n",
    "ecr_client.delete_repository(\n",
    "    repositoryName=launch_result.ecr_uri.split('/')[1],\n",
    "    force=True\n",
    ")\n",
    "\n",
    "# Clean up local files\n",
    "os.remove('large_sales_data.xlsx')\n",
    "os.remove('sales_chart.png')\n",
    "\n",
    "print(\"‚úÖ Cleanup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have successfully demonstrated handling large multi-modal payloads with Amazon Bedrock AgentCore Runtime!\n",
    "\n",
    "## What you've learned:\n",
    "\n",
    "### Large Payload Processing\n",
    "* **100MB Support**: AgentCore Runtime can handle payloads up to 100MB\n",
    "* **Base64 Encoding**: Secure transmission of binary data through JSON payloads\n",
    "* **Efficient Processing**: Runtime optimized for large data processing\n",
    "\n",
    "### Multi-Modal Capabilities\n",
    "* **Excel Analysis**: Processing structured data from spreadsheets\n",
    "* **Image Processing**: Analyzing visual content and charts\n",
    "* **Combined Analysis**: Correlating insights from multiple data types\n",
    "\n",
    "### Key Benefits\n",
    "* **Rich Data Processing**: Handle complex, multi-format datasets\n",
    "* **Scalable Architecture**: Runtime designed for large workloads\n",
    "* **Tool Integration**: Custom tools for specialized data processing\n",
    "* **Enterprise Ready**: Secure handling of sensitive business data\n",
    "\n",
    "This demonstrates AgentCore Runtime's capability to handle enterprise-scale data processing tasks with multiple data modalities, making it ideal for complex business intelligence and data analysis applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
