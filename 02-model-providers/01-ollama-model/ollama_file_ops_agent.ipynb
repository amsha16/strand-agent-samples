{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Local Agent with Strands Agents and Ollama Model\n",
    "\n",
    "This notebook demonstrates how to create a personal agent using Strands Agent and Ollama. The agent will be capable of performing various local tasks such as file operations, web searches, and system commands.\n",
    "\n",
    "## What is Ollama?\n",
    "\n",
    "[Ollama](https://ollama.com/) is an open-source framework that allows you to run large language models (LLMs) locally on your machine. It provides a simple API for interacting with these models, making it ideal for privacy-focused applications where you don't want to send data to external services.\n",
    "\n",
    "Key benefits of Ollama:\n",
    "- **Privacy**: All processing happens locally on your machine\n",
    "- **No API costs**: Free to use as much as you want\n",
    "- **Offline capability**: Works without internet connection\n",
    "- **Customization**: Can be fine-tuned for specific use \n",
    "\n",
    "\n",
    "## Agent Details\n",
    "\n",
    "<div style=\"float: left; margin-right: 20px;\">\n",
    "    \n",
    "|Feature             |Description                                        |\n",
    "|--------------------|---------------------------------------------------|\n",
    "|Feature used        |Ollama Model - to create a file operations agent   |\n",
    "|Agent Structure     |Single agent architecture                          |\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Architecture\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/architecture.png\" width=\"65%\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: strands-agents in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: strands-agents-tools in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (0.1.9)\n",
      "Collecting PyPDF2>=3.0.0 (from -r requirements.txt (line 4))\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.37.3)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.29.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.37.3)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (0.17.0)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.8.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.30.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-threading<1.00b0,>=0.51b0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (0.59b0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.30.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.38.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (2.12.4)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7.0.0,>=6.0.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (6.0.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents->-r requirements.txt (line 1)) (0.11.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents->-r requirements.txt (line 1)) (1.26.20)\n",
      "Requirement already satisfied: anyio>=4.5 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (0.4.3)\n",
      "Requirement already satisfied: httpx>=0.27.1 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (4.23.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (2.12.0)\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in /opt/conda/lib/python3.12/site-packages (from pyjwt[crypto]>=2.10.1->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (0.0.20)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: starlette>=0.27 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (0.49.2)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (0.38.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents->-r requirements.txt (line 1)) (6.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.30.0->strands-agents->-r requirements.txt (line 1)) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.59b0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents->-r requirements.txt (line 1)) (0.59b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents->-r requirements.txt (line 1)) (1.17.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents->-r requirements.txt (line 1)) (0.59b0)\n",
      "Requirement already satisfied: packaging>=18.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->strands-agents->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->strands-agents->-r requirements.txt (line 1)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->strands-agents->-r requirements.txt (line 1)) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.29.0->strands-agents->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: aws-requests-auth<0.5.0,>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (0.4.3)\n",
      "Requirement already satisfied: dill<0.5.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: pillow<12.0.0,>=11.2.1 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (11.3.0)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.51 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (3.0.52)\n",
      "Requirement already satisfied: rich<15.0.0,>=14.0.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (14.2.0)\n",
      "Requirement already satisfied: slack-bolt<2.0.0,>=1.23.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (1.25.0)\n",
      "Requirement already satisfied: sympy<2.0.0,>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=9.1.2 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (9.1.2)\n",
      "Requirement already satisfied: requests>=0.14.0 in /opt/conda/lib/python3.12/site-packages (from aws-requests-auth<0.5.0,>=0.4.3->strands-agents-tools->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.51->strands-agents-tools->-r requirements.txt (line 2)) (0.2.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich<15.0.0,>=14.0.0->strands-agents-tools->-r requirements.txt (line 2)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich<15.0.0,>=14.0.0->strands-agents-tools->-r requirements.txt (line 2)) (2.19.2)\n",
      "Requirement already satisfied: slack_sdk<4,>=3.35.0 in /opt/conda/lib/python3.12/site-packages (from slack-bolt<2.0.0,>=1.23.0->strands-agents-tools->-r requirements.txt (line 2)) (3.37.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy<2.0.0,>=1.12.0->strands-agents-tools->-r requirements.txt (line 2)) (1.3.0)\n",
      "Collecting ollama<1.0.0,>=0.4.8 (from strands-agents[ollama]->-r requirements.txt (line 3))\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27.1->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27.1->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (0.28.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=14.0.0->strands-agents-tools->-r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from pydantic-settings>=2.5.2->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /opt/conda/lib/python3.12/site-packages (from pyjwt[crypto]>=2.10.1->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (46.0.3)\n",
      "Requirement already satisfied: cffi>=1.14 in /opt/conda/lib/python3.12/site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.12/site-packages (from cffi>=1.14->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (2.22)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=0.14.0->aws-requests-auth<0.5.0,>=0.4.3->strands-agents-tools->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn>=0.31.1->mcp<2.0.0,>=1.8.0->strands-agents->-r requirements.txt (line 1)) (8.3.0)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2, ollama\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [ollama]\n",
      "\u001b[1A\u001b[2KSuccessfully installed PyPDF2-3.0.1 ollama-0.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "Before running this notebook, make sure you have:\n",
    "\n",
    "1. Install Ollama: Follow instructions at [https://ollama.com/download](https://ollama.com/download)\n",
    "2. Start the Ollama server: `ollama serve`\n",
    "3. Downloaded a model with Ollama: `ollama pull llama3.2:1b`\n",
    "\n",
    "Refer to the [documentation](https://cuddly-sniffle-lrmk2y7.pages.github.io/0.1.x/user-guide/concepts/model-providers/ollama/) for detailed instructions.\n",
    "\n",
    "In this notebook, we will download Ollama for the linux distribution for compatibility with SageMaker Studio. This is done for code execution during AWS lead workshops on Workshop Studio. If you are running this code locally, you should adjust the steps to download Ollama to your current enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "# this will work on linux computers\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['ollama', 'serve']>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find '/home/sagemaker-user/.ollama/id_ed25519'. Generating new private key.\n",
      "Your new public key is: \n",
      "\n",
      "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIKev77QEGhCaHxdL4Zt1mbQCMcLp2lvSyrCKalrprW+c\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-11-28T09:03:38.310Z level=INFO source=routes.go:1544 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/sagemaker-user/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2025-11-28T09:03:38.323Z level=INFO source=images.go:522 msg=\"total blobs: 0\"\n",
      "time=2025-11-28T09:03:38.323Z level=INFO source=images.go:529 msg=\"total unused blobs removed: 0\"\n",
      "time=2025-11-28T09:03:38.328Z level=INFO source=routes.go:1597 msg=\"Listening on 127.0.0.1:11434 (version 0.13.0)\"\n",
      "time=2025-11-28T09:03:38.328Z level=INFO source=runner.go:67 msg=\"discovering available GPUs...\"\n",
      "time=2025-11-28T09:03:38.328Z level=INFO source=runner.go:102 msg=\"experimental Vulkan support disabled.  To enable, set OLLAMA_VULKAN=1\"\n",
      "time=2025-11-28T09:03:38.340Z level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --port 36107\"\n",
      "time=2025-11-28T09:03:38.394Z level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --port 35179\"\n",
      "time=2025-11-28T09:03:38.425Z level=INFO source=types.go:60 msg=\"inference compute\" id=cpu library=cpu compute=\"\" name=cpu description=cpu libdirs=ollama driver=\"\" pci_id=\"\" type=\"\" total=\"31.0 GiB\" available=\"26.3 GiB\"\n",
      "time=2025-11-28T09:03:38.425Z level=INFO source=routes.go:1638 msg=\"entering low vram mode\" \"total vram\"=\"0 B\" threshold=\"20.0 GiB\"\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.Popen(['ollama', 'serve'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/11/28 - 09:10:21 | 200 |      26.988µs |       127.0.0.1 | HEAD     \"/\"\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l[GIN] 2025/11/28 - 09:10:21 | 200 |  433.347447ms |       127.0.0.1 | POST     \"/api/pull\"\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "# Import strands components\n",
    "from strands import Agent, tool\n",
    "from strands.models.ollama import OllamaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/11/28 - 09:11:02 | 200 |     779.219µs |       127.0.0.1 | GET      \"/api/tags\"\n",
      "✅ Ollama is running. Available models:\n",
      "- llama3.2:latest\n",
      "- llama3.2:3b\n"
     ]
    }
   ],
   "source": [
    "# Check if Ollama is running:\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "    print(\"✅ Ollama is running. Available models:\")\n",
    "    for model in response.json().get(\"models\", []):\n",
    "        print(f\"- {model['name']}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"❌ Ollama is not running. Please start Ollama before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.models.ollama import OllamaModel\n",
    "\n",
    "# Configure the Ollama model\n",
    "ollama_model = OllamaModel(\n",
    "    model_id=\"llama3.2:latest\", # Make sure this is the model you downloaded from Ollama\n",
    "    host=\"http://localhost:11434\",\n",
    "    params={\n",
    "        \"max_tokens\": 2048,  # Adjust based on your model's capabilities\n",
    "        \"temperature\": 0.7,  # Lower for more deterministic responses, higher for more creative\n",
    "        \"top_p\": 0.9,        # Nucleus sampling parameter\n",
    "        \"stream\": True,      # Enable streaming responses\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Custom Tools\n",
    "\n",
    "Tools are functions that the agent can use to interact with the environment. Below, we define several useful tools for our personal agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Operation Tools\n",
    "@tool\n",
    "def file_read(file_path: str) -> str:\n",
    "    \"\"\"Read a file and return its content. Supports both text and PDF files.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file to read\n",
    "\n",
    "    Returns:\n",
    "        str: Content of the file\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file doesn't exist\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if it's a PDF file\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            import PyPDF2\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text if text.strip() else \"Error: Could not extract text from PDF\"\n",
    "        else:\n",
    "            # Regular text file\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File '{file_path}' not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def file_write(file_path: str, content: str) -> str:\n",
    "    \"\"\"Write content to a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file\n",
    "        content (str): The content to write to the file\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating success or failure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(file_path)), exist_ok=True)\n",
    "\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(content)\n",
    "        return f\"File '{file_path}' written successfully.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing to file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_directory(directory_path: str = \".\") -> str:\n",
    "    \"\"\"List files and directories in the specified path.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory to list\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string listing all files and directories\n",
    "    \"\"\"\n",
    "    try:\n",
    "        items = os.listdir(directory_path)\n",
    "        files = []\n",
    "        directories = []\n",
    "\n",
    "        for item in items:\n",
    "            full_path = os.path.join(directory_path, item)\n",
    "            if os.path.isdir(full_path):\n",
    "                directories.append(f\"Folder: {item}/\")\n",
    "            else:\n",
    "                files.append(f\"File: {item}\")\n",
    "\n",
    "        result = f\"Contents of {os.path.abspath(directory_path)}:\\n\"\n",
    "        result += (\n",
    "            \"\\nDirectories:\\n\" + \"\\n\".join(sorted(directories))\n",
    "            if directories\n",
    "            else \"\\nNo directories found.\"\n",
    "        )\n",
    "        result += (\n",
    "            \"\\n\\nFiles:\\n\" + \"\\n\".join(sorted(files)) if files else \"\\nNo files found.\"\n",
    "        )\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Error listing directory: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Ollama-powered Agent\n",
    "\n",
    "Now we'll create our agent using the Ollama model and the tools we defined above.\n",
    "\n",
    "Note: You can add more tools like `execute_commands`, `search_files` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a comprehensive system prompt for our agent\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful personal assistant capable of performing local file actions and simple tasks for the user.\n",
    "\n",
    "Your key capabilities:\n",
    "1. Read, understand, and summarize files.\n",
    "2. Create and write to files.\n",
    "3. List directory contents and provide information on the files.\n",
    "4. Summarize text content\n",
    "\n",
    "When using tools:\n",
    "- Always verify file paths before operations\n",
    "- Be careful with system commands\n",
    "- Provide clear explanations of what you're doing\n",
    "- If a task cannot be completed, explain why and suggest alternatives\n",
    "\n",
    "Always be helpful, concise, and focus on addressing the user's needs efficiently.\n",
    "\"\"\"\n",
    "\n",
    "model_id = (\n",
    "    \"llama3.2:3b\"  # You can change this to any model you have pulled with Ollama.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent with tools\n",
    "local_agent = Agent(\n",
    "    system_prompt=system_prompt, # Define a system Prompt\n",
    "    model=ollama_model,\n",
    "    tools=[file_read, file_write, list_directory],  # Add your custom tools here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Agent\n",
    "\n",
    "Let's test our agent with some example tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-11-28T09:15:07.623Z level=WARN source=cpu_linux.go:130 msg=\"failed to parse CPU allowed micro secs\" error=\"strconv.ParseInt: parsing \\\"max\\\": invalid syntax\"\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/sagemaker-user/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.87 GiB (5.01 BPW) \n",
      "load: printing all EOG tokens:\n",
      "load:   - 128001 ('<|end_of_text|>')\n",
      "load:   - 128008 ('<|eom_id|>')\n",
      "load:   - 128009 ('<|eot_id|>')\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 1\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Llama 3.2 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "llama_model_load: vocab only - skipping tensors\n",
      "time=2025-11-28T09:15:08.119Z level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --model /home/sagemaker-user/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --port 45159\"\n",
      "time=2025-11-28T09:15:08.120Z level=INFO source=sched.go:443 msg=\"system memory\" total=\"31.0 GiB\" free=\"24.7 GiB\" free_swap=\"0 B\"\n",
      "time=2025-11-28T09:15:08.120Z level=INFO source=server.go:459 msg=\"loading model\" \"model layers\"=29 requested=-1\n",
      "time=2025-11-28T09:15:08.120Z level=INFO source=device.go:245 msg=\"model weights\" device=CPU size=\"1.9 GiB\"\n",
      "time=2025-11-28T09:15:08.121Z level=INFO source=device.go:256 msg=\"kv cache\" device=CPU size=\"448.0 MiB\"\n",
      "time=2025-11-28T09:15:08.121Z level=INFO source=device.go:272 msg=\"total memory\" size=\"2.3 GiB\"\n",
      "time=2025-11-28T09:15:08.133Z level=INFO source=runner.go:963 msg=\"starting go runner\"\n",
      "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
      "time=2025-11-28T09:15:08.141Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)\n",
      "time=2025-11-28T09:15:08.141Z level=INFO source=runner.go:999 msg=\"Server listening on 127.0.0.1:45159\"\n",
      "time=2025-11-28T09:15:08.143Z level=INFO source=runner.go:893 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:4 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-11-28T09:15:08.143Z level=INFO source=server.go:1294 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-11-28T09:15:08.144Z level=INFO source=server.go:1328 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/sagemaker-user/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.87 GiB (5.01 BPW) \n",
      "load: printing all EOG tokens:\n",
      "load:   - 128001 ('<|end_of_text|>')\n",
      "load:   - 128008 ('<|eom_id|>')\n",
      "load:   - 128009 ('<|eot_id|>')\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 24\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 3\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Llama 3.2 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = false)\n",
      "load_tensors:          CPU model buffer size =  1918.35 MiB\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = disabled\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_context:        CPU  output buffer size =     0.50 MiB\n",
      "llama_kv_cache:        CPU KV buffer size =   448.00 MiB\n",
      "llama_kv_cache: size =  448.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  224.00 MiB, V (f16):  224.00 MiB\n",
      "llama_context:        CPU compute buffer size =   256.50 MiB\n",
      "llama_context: graph nodes  = 1014\n",
      "llama_context: graph splits = 1\n",
      "time=2025-11-28T09:15:10.150Z level=INFO source=server.go:1332 msg=\"llama runner started in 2.03 seconds\"\n",
      "time=2025-11-28T09:15:10.150Z level=INFO source=sched.go:517 msg=\"loaded runners\" count=1\n",
      "time=2025-11-28T09:15:10.150Z level=INFO source=server.go:1294 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-11-28T09:15:10.150Z level=INFO source=server.go:1332 msg=\"llama runner started in 2.03 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #1: list_directory\n",
      "[GIN] 2025/11/28 - 09:15:35 | 200 | 28.143890485s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "The current directory contains the following files and directories:\n",
      "\n",
      "* A single Python notebook file named `ollama_file_ops_agent.ipynb`\n",
      "* A text file named `requirements.txt` containing dependencies\n",
      "* An `images` directory, possibly for storing images used in the model\n",
      "* A `.ipynb_checkpoints` directory, which likely stores checkpoints for the Jupyter Notebook \n",
      "\n",
      "Would you like to read or write any of these files? Or perhaps perform an operation on one of the directories?[GIN] 2025/11/28 - 09:15:50 | 200 | 15.010556706s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentResult(stop_reason='end_turn', message={'role': 'assistant', 'content': [{'text': 'The current directory contains the following files and directories:\\n\\n* A single Python notebook file named `ollama_file_ops_agent.ipynb`\\n* A text file named `requirements.txt` containing dependencies\\n* An `images` directory, possibly for storing images used in the model\\n* A `.ipynb_checkpoints` directory, which likely stores checkpoints for the Jupyter Notebook \\n\\nWould you like to read or write any of these files? Or perhaps perform an operation on one of the directories?'}]}, metrics=EventLoopMetrics(cycle_count=2, tool_metrics={'list_directory': ToolMetrics(tool={'toolUseId': 'list_directory', 'name': 'list_directory', 'input': {'directory_path': '.'}}, call_count=1, success_count=1, error_count=0, total_time=0.0005338191986083984)}, cycle_durations=[15.012617826461792], traces=[<strands.telemetry.metrics.Trace object at 0x7f34b844d1f0>, <strands.telemetry.metrics.Trace object at 0x7f34ba170170>], accumulated_usage={'inputTokens': 117, 'outputTokens': 810, 'totalTokens': 927}, accumulated_metrics={'latencyMs': 43154.324354000004}), state={})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_agent(\"Show me the files in the current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #2: file_write\n",
      "\n",
      "Tool #3: file_write\n",
      "[GIN] 2025/11/28 - 09:16:53 | 200 | 24.783465127s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "The file `sample.txt` has been created in the current directory with the specified content. The file now contains:\n",
      "\n",
      "```\n",
      "This is a test file created by my Ollama agent.\n",
      "```[GIN] 2025/11/28 - 09:17:03 | 200 |  9.558477298s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentResult(stop_reason='end_turn', message={'role': 'assistant', 'content': [{'text': 'The file `sample.txt` has been created in the current directory with the specified content. The file now contains:\\n\\n```\\nThis is a test file created by my Ollama agent.\\n```'}]}, metrics=EventLoopMetrics(cycle_count=5, tool_metrics={'list_directory': ToolMetrics(tool={'toolUseId': 'list_directory', 'name': 'list_directory', 'input': {'directory_path': '.'}}, call_count=1, success_count=1, error_count=0, total_time=0.0005338191986083984), 'file_write': ToolMetrics(tool={'toolUseId': 'file_write', 'name': 'file_write', 'input': {'content': 'This is a test file created by my Ollama agent.', 'file_path': 'sample.txt'}}, call_count=2, success_count=2, error_count=0, total_time=0.0023565292358398438)}, cycle_durations=[15.012617826461792, 9.560912609100342], traces=[<strands.telemetry.metrics.Trace object at 0x7f34b844d1f0>, <strands.telemetry.metrics.Trace object at 0x7f34ba170170>, <strands.telemetry.metrics.Trace object at 0x7f34b86a81d0>, <strands.telemetry.metrics.Trace object at 0x7f34ac4921b0>, <strands.telemetry.metrics.Trace object at 0x7f34b92c49e0>], accumulated_usage={'inputTokens': 217, 'outputTokens': 2093, 'totalTokens': 2310}, accumulated_metrics={'latencyMs': 77496.138793}), state={})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_agent(\"Create a file called 'sample.txt' with the content 'This is a test file created by my Ollama agent.'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test file created by my Ollama agent."
     ]
    }
   ],
   "source": [
    "!cat sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #4: file_read\n",
      "[GIN] 2025/11/28 - 09:19:55 | 200 | 20.085597099s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "It appears that there is no file named 'document.txt' in the current directory.\n",
      "\n",
      "Would you like to create a new file with this name, or perhaps specify a different file path? I can help you with either of these options.[GIN] 2025/11/28 - 09:20:04 | 200 |  8.802161933s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentResult(stop_reason='end_turn', message={'role': 'assistant', 'content': [{'text': \"It appears that there is no file named 'document.txt' in the current directory.\\n\\nWould you like to create a new file with this name, or perhaps specify a different file path? I can help you with either of these options.\"}]}, metrics=EventLoopMetrics(cycle_count=8, tool_metrics={'list_directory': ToolMetrics(tool={'toolUseId': 'list_directory', 'name': 'list_directory', 'input': {'directory_path': '.'}}, call_count=1, success_count=1, error_count=0, total_time=0.0005338191986083984), 'file_write': ToolMetrics(tool={'toolUseId': 'file_write', 'name': 'file_write', 'input': {'content': 'This is a test file created by my Ollama agent.', 'file_path': 'sample.txt'}}, call_count=2, success_count=2, error_count=0, total_time=0.0023565292358398438), 'file_read': ToolMetrics(tool={'toolUseId': 'file_read', 'name': 'file_read', 'input': {'file_path': 'document.txt'}}, call_count=1, success_count=1, error_count=0, total_time=0.0004456043243408203)}, cycle_durations=[15.012617826461792, 9.560912609100342, 8.804601430892944], traces=[<strands.telemetry.metrics.Trace object at 0x7f34b844d1f0>, <strands.telemetry.metrics.Trace object at 0x7f34ba170170>, <strands.telemetry.metrics.Trace object at 0x7f34b86a81d0>, <strands.telemetry.metrics.Trace object at 0x7f34ac4921b0>, <strands.telemetry.metrics.Trace object at 0x7f34b92c49e0>, <strands.telemetry.metrics.Trace object at 0x7f34ac4b11c0>, <strands.telemetry.metrics.Trace object at 0x7f34b86a95b0>, <strands.telemetry.metrics.Trace object at 0x7f34b8e1a990>], accumulated_usage={'inputTokens': 284, 'outputTokens': 3641, 'totalTokens': 3925}, accumulated_metrics={'latencyMs': 106383.751855}), state={})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Trying with some unknown file\n",
    "local_agent(\"Read the file 'document.txt' and summarize it in 5 bullet points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #5: file_read\n",
      "[GIN] 2025/11/28 - 09:21:00 | 200 | 20.336074956s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "Here are five bullet points summarizing the content of 'sample.txt':\n",
      "\n",
      "* The file 'sample.txt' was created by an Ollama agent.\n",
      "* The purpose of the file appears to be demonstration or testing.\n",
      "* No specific information about the user is provided.\n",
      "* The file does not contain any additional context or details.\n",
      "* It serves as a starting point for further development or exploration.[GIN] 2025/11/28 - 09:21:13 | 200 | 12.150845809s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentResult(stop_reason='end_turn', message={'role': 'assistant', 'content': [{'text': \"Here are five bullet points summarizing the content of 'sample.txt':\\n\\n* The file 'sample.txt' was created by an Ollama agent.\\n* The purpose of the file appears to be demonstration or testing.\\n* No specific information about the user is provided.\\n* The file does not contain any additional context or details.\\n* It serves as a starting point for further development or exploration.\"}]}, metrics=EventLoopMetrics(cycle_count=11, tool_metrics={'list_directory': ToolMetrics(tool={'toolUseId': 'list_directory', 'name': 'list_directory', 'input': {'directory_path': '.'}}, call_count=1, success_count=1, error_count=0, total_time=0.0005338191986083984), 'file_write': ToolMetrics(tool={'toolUseId': 'file_write', 'name': 'file_write', 'input': {'content': 'This is a test file created by my Ollama agent.', 'file_path': 'sample.txt'}}, call_count=2, success_count=2, error_count=0, total_time=0.0023565292358398438), 'file_read': ToolMetrics(tool={'toolUseId': 'file_read', 'name': 'file_read', 'input': {'file_path': 'sample.txt'}}, call_count=2, success_count=2, error_count=0, total_time=0.0008945465087890625)}, cycle_durations=[15.012617826461792, 9.560912609100342, 8.804601430892944, 12.153214693069458], traces=[<strands.telemetry.metrics.Trace object at 0x7f34b844d1f0>, <strands.telemetry.metrics.Trace object at 0x7f34ba170170>, <strands.telemetry.metrics.Trace object at 0x7f34b86a81d0>, <strands.telemetry.metrics.Trace object at 0x7f34ac4921b0>, <strands.telemetry.metrics.Trace object at 0x7f34b92c49e0>, <strands.telemetry.metrics.Trace object at 0x7f34ac4b11c0>, <strands.telemetry.metrics.Trace object at 0x7f34b86a95b0>, <strands.telemetry.metrics.Trace object at 0x7f34b8e1a990>, <strands.telemetry.metrics.Trace object at 0x7f34adb5c950>, <strands.telemetry.metrics.Trace object at 0x7f34ae62ec90>, <strands.telemetry.metrics.Trace object at 0x7f34ae62d220>], accumulated_usage={'inputTokens': 382, 'outputTokens': 5446, 'totalTokens': 5828}, accumulated_metrics={'latencyMs': 138870.544871}), state={})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Trying with sample.txt file\n",
    "local_agent(\"Read the file 'sample.txt' and summarize it in 5 bullet points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #6: file_read\n",
      "[GIN] 2025/11/28 - 09:24:12 | 200 |  23.56660999s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-11-28T09:24:12.714Z level=WARN source=runner.go:152 msg=\"truncating input prompt\" limit=4096 prompt=9380 keep=5 new=4096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andy Jassy, the current CEO of Amazon, provided an overview of the company's past, present, and future in his 2021 letter to Amazon shareholders. Here's a summary:\n",
      "\n",
      "**Historical Context**: The letter begins by referencing Amazon's humble beginnings as an online bookstore. In 1997, Jeff Bezos had just launched the company and was facing challenges from established players in the retail industry.\n",
      "\n",
      "**Key Milestones**:\n",
      "\n",
      "* By year-end 1997, Amazon had served over 1.5 million customers, with revenue growth of 838% to $147.8 million.\n",
      "* The company's employee base grew from 158 to 614, and its distribution center capacity expanded to 285,000 square feet.\n",
      "\n",
      "**Focus on Customers**: Bezos emphasized the importance of customer satisfaction, citing the company's focus on providing a better shopping experience through features like gift certificates, 1-Click shopping, and vast product selection. He noted that word-of-mouth referrals remained a key driver of customer acquisition.\n",
      "\n",
      "**Strategic Priorities**: In 1998, Amazon aimed to solidify its brand and expand into new areas, such as music sales and international markets. The company recognized the need for continued investment in infrastructure and systems to support growing demand.\n",
      "\n",
      "**Challenges and Hurdles**: Bezos acknowledged the challenges facing Amazon, including aggressive competition, growth risks, and the need for large investments to meet expanding market opportunities.\n",
      "\n",
      "**Long-term Vision**: Despite the challenges, Bezos expressed optimism about Amazon's long-term prospects. He emphasized the company's focus on innovation, customer obsession, and ownership culture.\n",
      "\n",
      "The letter concluded with a message from Bezos himself, reiterating his commitment to Amazon's customers, employees, and shareholders.[GIN] 2025/11/28 - 09:28:25 | 200 |         4m13s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentResult(stop_reason='end_turn', message={'role': 'assistant', 'content': [{'text': \"Andy Jassy, the current CEO of Amazon, provided an overview of the company's past, present, and future in his 2021 letter to Amazon shareholders. Here's a summary:\\n\\n**Historical Context**: The letter begins by referencing Amazon's humble beginnings as an online bookstore. In 1997, Jeff Bezos had just launched the company and was facing challenges from established players in the retail industry.\\n\\n**Key Milestones**:\\n\\n* By year-end 1997, Amazon had served over 1.5 million customers, with revenue growth of 838% to $147.8 million.\\n* The company's employee base grew from 158 to 614, and its distribution center capacity expanded to 285,000 square feet.\\n\\n**Focus on Customers**: Bezos emphasized the importance of customer satisfaction, citing the company's focus on providing a better shopping experience through features like gift certificates, 1-Click shopping, and vast product selection. He noted that word-of-mouth referrals remained a key driver of customer acquisition.\\n\\n**Strategic Priorities**: In 1998, Amazon aimed to solidify its brand and expand into new areas, such as music sales and international markets. The company recognized the need for continued investment in infrastructure and systems to support growing demand.\\n\\n**Challenges and Hurdles**: Bezos acknowledged the challenges facing Amazon, including aggressive competition, growth risks, and the need for large investments to meet expanding market opportunities.\\n\\n**Long-term Vision**: Despite the challenges, Bezos expressed optimism about Amazon's long-term prospects. He emphasized the company's focus on innovation, customer obsession, and ownership culture.\\n\\nThe letter concluded with a message from Bezos himself, reiterating his commitment to Amazon's customers, employees, and shareholders.\"}]}, metrics=EventLoopMetrics(cycle_count=14, tool_metrics={'list_directory': ToolMetrics(tool={'toolUseId': 'list_directory', 'name': 'list_directory', 'input': {'directory_path': '.'}}, call_count=1, success_count=1, error_count=0, total_time=0.0005338191986083984), 'file_write': ToolMetrics(tool={'toolUseId': 'file_write', 'name': 'file_write', 'input': {'content': 'This is a test file created by my Ollama agent.', 'file_path': 'sample.txt'}}, call_count=2, success_count=2, error_count=0, total_time=0.0023565292358398438), 'file_read': ToolMetrics(tool={'toolUseId': 'file_read', 'name': 'file_read', 'input': {'file_path': 'sample_file/Amazon-com-Inc-2023-Shareholder-Letter.pdf'}}, call_count=3, success_count=3, error_count=0, total_time=0.23648858070373535)}, cycle_durations=[15.012617826461792, 9.560912609100342, 8.804601430892944, 12.153214693069458, 253.30500507354736], traces=[<strands.telemetry.metrics.Trace object at 0x7f34b844d1f0>, <strands.telemetry.metrics.Trace object at 0x7f34ba170170>, <strands.telemetry.metrics.Trace object at 0x7f34b86a81d0>, <strands.telemetry.metrics.Trace object at 0x7f34ac4921b0>, <strands.telemetry.metrics.Trace object at 0x7f34b92c49e0>, <strands.telemetry.metrics.Trace object at 0x7f34ac4b11c0>, <strands.telemetry.metrics.Trace object at 0x7f34b86a95b0>, <strands.telemetry.metrics.Trace object at 0x7f34b8e1a990>, <strands.telemetry.metrics.Trace object at 0x7f34adb5c950>, <strands.telemetry.metrics.Trace object at 0x7f34ae62ec90>, <strands.telemetry.metrics.Trace object at 0x7f34ae62d220>, <strands.telemetry.metrics.Trace object at 0x7f34aff98530>, <strands.telemetry.metrics.Trace object at 0x7f34aff981d0>, <strands.telemetry.metrics.Trace object at 0x7f34ae663770>], accumulated_usage={'inputTokens': 768, 'outputTokens': 10794, 'totalTokens': 11562}, accumulated_metrics={'latencyMs': 415737.797539}), state={})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_agent(\n",
    "    \"Read the file in the path `sample_file/Amazon-com-Inc-2023-Shareholder-Letter.pdf` and summarize it in 5 bullet points.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #7: file_write\n",
      "\n",
      "Tool #8: file_write\n",
      "[GIN] 2025/11/28 - 09:31:02 | 200 | 52.648088741s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "Here is the complete README.md file for the current directory:\n",
      "\n",
      "```\n",
      "# README\n",
      "## Description\n",
      "This is the root directory of our project.\n",
      "\n",
      "## Getting Started\n",
      "To get started, navigate to this directory and run `npm install`.\n",
      "```[GIN] 2025/11/28 - 09:31:12 | 200 | 10.418170696s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    }
   ],
   "source": [
    "# Example 4: create a readme file after reading and understanding multiple files\n",
    "response = local_agent(\"Create a readme.md for the current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try to configure Web Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful personal assistant capable of performing web search for technology related updates, local file actions and simple tasks for the user.\n",
    "\n",
    "Your key capabilities:\n",
    "1. Read, understand, and summarize files.\n",
    "2. Create and write to files.\n",
    "3. List directory contents and provide information on the files.\n",
    "4. Summarize text content\n",
    "5. Sacn websites and mention sources of information\n",
    "\n",
    "When using tools:\n",
    "- Always verify file paths before operations\n",
    "- Be careful with system commands\n",
    "- Provide clear explanations of what you're doing\n",
    "- If a task cannot be completed, explain why and suggest alternatives\n",
    "\n",
    "Always be helpful, concise, and focus on addressing the user's needs efficiently.\n",
    "\"\"\"\n",
    "\n",
    "model_id = (\n",
    "    \"llama3.2:3b\"  # You can change this to any model you have pulled with Ollama.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tool=<<bound method Client.web_search of <ollama._client.Client object at 0x7f34b9701ac0>>> | unrecognized tool specification\n"
     ]
    }
   ],
   "source": [
    "# Create the agent with tools\n",
    "\n",
    "from ollama import web_fetch, web_search\n",
    "\n",
    "local_agent = Agent(\n",
    "    system_prompt=system_prompt, # Define a system Prompt\n",
    "    model=ollama_model,\n",
    "    tools=[file_read, file_write, list_directory, web_search],  # Add your custom tools here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-11-28T09:39:32.716Z level=WARN source=cpu_linux.go:130 msg=\"failed to parse CPU allowed micro secs\" error=\"strconv.ParseInt: parsing \\\"max\\\": invalid syntax\"\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/sagemaker-user/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.87 GiB (5.01 BPW) \n",
      "load: printing all EOG tokens:\n",
      "load:   - 128001 ('<|end_of_text|>')\n",
      "load:   - 128008 ('<|eom_id|>')\n",
      "load:   - 128009 ('<|eot_id|>')\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 1\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Llama 3.2 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "llama_model_load: vocab only - skipping tensors\n",
      "time=2025-11-28T09:39:33.191Z level=INFO source=server.go:392 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --model /home/sagemaker-user/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --port 36099\"\n",
      "time=2025-11-28T09:39:33.191Z level=INFO source=sched.go:443 msg=\"system memory\" total=\"31.0 GiB\" free=\"25.0 GiB\" free_swap=\"0 B\"\n",
      "time=2025-11-28T09:39:33.191Z level=INFO source=server.go:459 msg=\"loading model\" \"model layers\"=29 requested=-1\n",
      "time=2025-11-28T09:39:33.192Z level=INFO source=device.go:245 msg=\"model weights\" device=CPU size=\"1.9 GiB\"\n",
      "time=2025-11-28T09:39:33.192Z level=INFO source=device.go:256 msg=\"kv cache\" device=CPU size=\"448.0 MiB\"\n",
      "time=2025-11-28T09:39:33.192Z level=INFO source=device.go:272 msg=\"total memory\" size=\"2.3 GiB\"\n",
      "time=2025-11-28T09:39:33.206Z level=INFO source=runner.go:963 msg=\"starting go runner\"\n",
      "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
      "time=2025-11-28T09:39:33.213Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)\n",
      "time=2025-11-28T09:39:33.214Z level=INFO source=runner.go:999 msg=\"Server listening on 127.0.0.1:36099\"\n",
      "time=2025-11-28T09:39:33.225Z level=INFO source=runner.go:893 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:4 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-11-28T09:39:33.225Z level=INFO source=server.go:1294 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-11-28T09:39:33.225Z level=INFO source=server.go:1328 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/sagemaker-user/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.87 GiB (5.01 BPW) \n",
      "load: printing all EOG tokens:\n",
      "load:   - 128001 ('<|end_of_text|>')\n",
      "load:   - 128008 ('<|eom_id|>')\n",
      "load:   - 128009 ('<|eot_id|>')\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 24\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 3\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Llama 3.2 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = false)\n",
      "load_tensors:          CPU model buffer size =  1918.35 MiB\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = disabled\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_context:        CPU  output buffer size =     0.50 MiB\n",
      "llama_kv_cache:        CPU KV buffer size =   448.00 MiB\n",
      "llama_kv_cache: size =  448.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  224.00 MiB, V (f16):  224.00 MiB\n",
      "llama_context:        CPU compute buffer size =   256.50 MiB\n",
      "llama_context: graph nodes  = 1014\n",
      "llama_context: graph splits = 1\n",
      "time=2025-11-28T09:39:34.981Z level=INFO source=server.go:1332 msg=\"llama runner started in 1.79 seconds\"\n",
      "time=2025-11-28T09:39:34.981Z level=INFO source=sched.go:517 msg=\"loaded runners\" count=1\n",
      "time=2025-11-28T09:39:34.981Z level=INFO source=server.go:1294 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-11-28T09:39:34.981Z level=INFO source=server.go:1332 msg=\"llama runner started in 1.79 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #1: file_read\n",
      "[GIN] 2025/11/28 - 09:40:04 | 200 | 32.160558467s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "It seems that I was unable to access the requested file.\n",
      "\n",
      "However, I can suggest an alternative approach. AWS has made significant developments in Agent-based AI (also known as Multi-agent Systems or MAS) through its various services and offerings. Here are some of the latest updates:\n",
      "\n",
      "1. **Amazon SageMaker Autopilot**: This is a fully managed service that enables businesses to build, deploy, and manage multi-agent systems using pre-built templates and APIs.\n",
      "2. **AWS DeepRacer**: This is a cloud-based autonomous racing platform that uses agent-based AI to enable developers to build and train their own autonomous vehicle models.\n",
      "3. **Amazon Rekognition Edge**: This service allows developers to use pre-trained computer vision models, including those for object detection and face recognition, which can be used in multi-agent systems.\n",
      "4. **AWS IoT Core**: This is a managed cloud service that enables businesses to securely connect, manage, and analyze the behavior of their devices, which can be used in agent-based AI applications.\n",
      "\n",
      "These are just a few examples of the latest developments in AWS for Agent-based AI. For more information, I recommend checking out the official AWS documentation and blog posts on these topics.\n",
      "\n",
      "Would you like me to summarize any specific aspect of these developments or provide further information?[GIN] 2025/11/28 - 09:40:36 | 200 | 31.510057422s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentResult(stop_reason='end_turn', message={'role': 'assistant', 'content': [{'text': 'It seems that I was unable to access the requested file.\\n\\nHowever, I can suggest an alternative approach. AWS has made significant developments in Agent-based AI (also known as Multi-agent Systems or MAS) through its various services and offerings. Here are some of the latest updates:\\n\\n1. **Amazon SageMaker Autopilot**: This is a fully managed service that enables businesses to build, deploy, and manage multi-agent systems using pre-built templates and APIs.\\n2. **AWS DeepRacer**: This is a cloud-based autonomous racing platform that uses agent-based AI to enable developers to build and train their own autonomous vehicle models.\\n3. **Amazon Rekognition Edge**: This service allows developers to use pre-trained computer vision models, including those for object detection and face recognition, which can be used in multi-agent systems.\\n4. **AWS IoT Core**: This is a managed cloud service that enables businesses to securely connect, manage, and analyze the behavior of their devices, which can be used in agent-based AI applications.\\n\\nThese are just a few examples of the latest developments in AWS for Agent-based AI. For more information, I recommend checking out the official AWS documentation and blog posts on these topics.\\n\\nWould you like me to summarize any specific aspect of these developments or provide further information?'}]}, metrics=EventLoopMetrics(cycle_count=3, tool_metrics={'file_read': ToolMetrics(tool={'toolUseId': 'file_read', 'name': 'file_read', 'input': {'file_path': 'https://docs.aws.amazon.com/AWSAutoScaling/latest/autoscaling_dg/AWS_AutoScaling_Terms_and_Concepts.html'}}, call_count=1, success_count=1, error_count=0, total_time=0.0004150867462158203)}, cycle_durations=[31.512011766433716], traces=[<strands.telemetry.metrics.Trace object at 0x7f34ae1e7d40>, <strands.telemetry.metrics.Trace object at 0x7f34ae03cd70>, <strands.telemetry.metrics.Trace object at 0x7f34ae03e2d0>], accumulated_usage={'inputTokens': 302, 'outputTokens': 876, 'totalTokens': 1178}, accumulated_metrics={'latencyMs': 63670.478869}), state={})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_agent(\n",
    "    \"Whats are the latest developments in AWS for Agentic AI\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've created a local personal agent using Stands and Ollama. The agent can perform file operations (read, write, append) and Summarize/Analyze text\n",
    "\n",
    "This demonstrates the power of running AI models locally with Ollama, combined with the flexibility of strands' tool system. You can extend this agent by adding more tools or using different Ollama models based on your needs.\n",
    "\n",
    "### Next Steps (Ideas)\n",
    "\n",
    "- Try different Ollama models to see how they affect the agent's capabilities\n",
    "- Add more complex tools like web search, email sending, or calendar integration\n",
    "- Implement memory for the agent to remember past interactions\n",
    "- Create a simple UI for interacting with your agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
